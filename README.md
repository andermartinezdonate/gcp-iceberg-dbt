# gcp-iceberg-dbt

End-to-end **GCP lakehouse demo** using **Apache Iceberg**, **BigQuery**, and **dbt**.

This project demonstrates how to build a modern analytics stack on Google Cloud using an open table format (Iceberg) as the storage and versioning layer, BigQuery as the query engine, and dbt for analytics engineering.

---

## Architecture

**Flow**
Local / Scripted Data
â†“
Google Cloud Storage (GCS)
â†“
Apache Iceberg (table format, metadata, snapshots)
â†“
BigQuery (query & analytics)
â†“
dbt (staging â†’ marts, tests)

**Key ideas**
- Iceberg handles **table format, schema evolution, and time travel**
- BigQuery queries Iceberg tables directly
- dbt builds clean analytical models on top

---

## Repository Structure
gcp-iceberg-dbt/
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ architecture.md        # Lakehouse + Iceberg architecture explanation
â”‚
â”œâ”€â”€ bigquery/
â”‚   â””â”€â”€ sql/
â”‚       â”œâ”€â”€ 01_create_iceberg_tables.sql   # CREATE TABLE (Iceberg)
â”‚       â””â”€â”€ 02_load_data.sql               # LOAD DATA (Iceberg snapshots)
â”‚
â”œâ”€â”€ iceberg/
â”‚   â””â”€â”€ README.md               # Iceberg concepts: snapshots, time travel, notes
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ README.md               # dbt-on-Iceberg notes
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ generated/              # Generated Parquet data (gitignored, reproducible)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ bootstrap_repo.sh       # Optional repo setup helpers
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ generate_commerce_data_soft.py  # Parquet data generator
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



---

## What This Project Shows

- How to organize data in **GCS** for Iceberg
- How to create and manage **Iceberg tables**
- How to query Iceberg tables from **BigQuery**
- How to build **dbt staging and mart models**
- How Iceberg snapshots differ from dbt snapshots
- Reproducible pipelines (no generated data committed)

---

## Prerequisites

- Google Cloud project
- GCS bucket
- BigQuery dataset
- Python 3.9+
- dbt (BigQuery adapter)
- Permissions to create tables and query data

---

## How to Run (High Level)

1. Configure GCP credentials
2. Upload raw data to GCS
3. Create Iceberg tables
4. Query tables from BigQuery
5. Run dbt models

Detailed, step-by-step instructions are provided in each folder.

---

## Why Iceberg + BigQuery + dbt?

- **Open format** (no vendor lock-in)
- **Time travel & schema evolution**
- **Separation of storage and compute**
- **Production-grade analytics workflows**

This mirrors real-world modern data platforms.

---

## Status

ðŸš§ Work in progress  
Planned additions:
- Full GCS bootstrap scripts
- Iceberg snapshot/time-travel demos
- dbt tests and documentation


---

## Author

Ander Martinez Donate  
Data Engineering / Analytics Engineering
